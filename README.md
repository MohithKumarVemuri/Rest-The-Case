# âš– Rest the Case â€” Production-Grade RAG Chat Assistant

A fully-featured **Retrieval-Augmented Generation (RAG)** chat assistant for legal services. Built with Node.js, Express, React, and OpenAI. Answers user questions using a private knowledge base â€” grounded responses, zero hallucination.

---

## ğŸ— Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER BROWSER                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              React Frontend (Vite + CSS)                     â”‚   â”‚
â”‚  â”‚   ChatWindow â”‚ InputBar â”‚ Sidebar (Retrieval Debug)          â”‚   â”‚
â”‚  â”‚              localStorage â†’ sessionId                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚  POST /api/chat { sessionId, message }
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      EXPRESS SERVER (Node.js)                        â”‚
â”‚                                                                      â”‚
â”‚  â‘  Validate Input                                                   â”‚
â”‚  â‘¡ Embed Query  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º OpenAI Embeddings  â”‚
â”‚  â‘¢ Similarity Search (Cosine)                                       â”‚
â”‚       vector_store.json â—„â”€â”€ compare â”€â”€ queryVector                  â”‚
â”‚       filter score â‰¥ 0.5 â†’ Top 3 chunks                            â”‚
â”‚  â‘£ Build Augmented Prompt                                           â”‚
â”‚       [System] + [Context Chunks] + [History] + [Question]          â”‚
â”‚  â‘¤ Call LLM  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º OpenAI GPT-4o-miniâ”‚
â”‚  â‘¥ Persist History (in-memory, last 4 pairs)                       â”‚
â”‚  â‘¦ Return structured JSON response                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PRE-PROCESSING (one-time, run ingest.js):
  docs.json â†’ chunk (300â€“400 words, 50-word overlap)
            â†’ embed each chunk (text-embedding-3-small)
            â†’ save to vector_store.json
```

---

## ğŸ”„ RAG Workflow Explanation

### Step 1 â€” Ingestion (Offline)
1. `docs.json` contains 10 documents about firm policies, fees, case types, etc.
2. `ingest.js` splits each document into overlapping chunks (~400 words, 50-word overlap).
3. Each chunk is sent to OpenAI's `text-embedding-3-small` model to produce a 1536-dimensional vector.
4. All chunks + vectors are saved to `vector_store.json`.

### Step 2 â€” Runtime (Per Query)
1. User sends a message â†’ server receives `{ sessionId, message }`.
2. The user's message is embedded via the same OpenAI embeddings model.
3. Cosine similarity is computed between the query vector and every stored chunk vector.
4. Chunks with similarity â‰¥ 0.5 are kept; top 3 by score are selected.
5. The retrieved chunks are injected into the LLM system prompt.
6. The LLM is instructed to answer **only** from provided context.
7. Conversation history (last 4 pairs) is also included for multi-turn coherence.

---

## ğŸ“ Embedding Strategy

| Property | Value |
|---|---|
| Model | `text-embedding-3-small` |
| Dimensions | 1536 |
| Chunk size | ~400 words |
| Overlap | 50 words |
| Similarity metric | Cosine similarity |
| Threshold | 0.5 (configurable) |
| Top-K | 3 chunks |

**Why cosine similarity?**  
Cosine similarity measures the angle between vectors, making it invariant to vector magnitude. This means a short question like "refund policy?" maps to the same semantic space as longer document chunks â€” making it ideal for asymmetric retrieval (short query vs. long document).

**Why overlapping chunks?**  
Without overlap, important context at chunk boundaries is lost. A 50-word overlap ensures no sentence is ever completely isolated from its surrounding context.

---

## âœï¸ Prompt Design Reasoning

```
SYSTEM:
  You are a helpful and professional legal assistant for "Rest the Case" law firm.
  Answer the user's question using ONLY the context provided below.
  Do NOT make up information. If the context doesn't fully address the question, say so politely.
  
  RETRIEVED CONTEXT:
  [1] (Title, similarity: 0.821)
      <chunk content>
  [2] (Title, similarity: 0.763)
      <chunk content>
  [3] (Title, similarity: 0.702)
      <chunk content>

HISTORY:
  user: <previous message>
  assistant: <previous reply>
  ...

USER: <current question>
```

**Design decisions:**
- **Context-first grounding**: The LLM is explicitly told to use only the retrieved context. This prevents hallucination.
- **Fallback handling**: If no chunks exceed the threshold, the system prompt changes to a safe fallback message.
- **Low temperature (0.2)**: Minimizes creative deviation; the model stays factual and predictable.
- **History inclusion**: Last 4 message pairs enable follow-up questions without losing context.
- **Similarity scores in prompt**: Including scores helps the model (and developer) understand retrieval confidence.

---

## ğŸ—‚ Project Structure

```
rag-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ docs.json           # Raw knowledge base (10 documents)
â”‚   â”‚   â””â”€â”€ vector_store.json   # Generated: chunks + embeddings
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ ingest.js           # Pre-processing: chunk â†’ embed â†’ store
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ vector_math.js      # Cosine similarity, dot product, retrieval
â”‚   â”œâ”€â”€ server.js               # Express server + RAG pipeline
â”‚   â”œâ”€â”€ .env.example            # Environment variable template
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatWindow.jsx  # Message list + scroll
â”‚   â”‚   â”‚   â”œâ”€â”€ Message.jsx     # Individual message bubble
â”‚   â”‚   â”‚   â”œâ”€â”€ InputBar.jsx    # Text input + send button
â”‚   â”‚   â”‚   â””â”€â”€ Sidebar.jsx     # Session info + retrieval debug
â”‚   â”‚   â”œâ”€â”€ App.jsx             # Root component + API calls
â”‚   â”‚   â”œâ”€â”€ main.jsx            # React entry point
â”‚   â”‚   â””â”€â”€ index.css           # Full design system
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md
```

---

## ğŸš€ Setup Instructions

### Prerequisites
- Node.js v18+
- An OpenAI API key

### 1. Clone & Install

```bash
# Install backend dependencies
cd backend
npm install

# Install frontend dependencies
cd ../frontend
npm install
```

### 2. Configure Environment

```bash
cd backend
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY
```

### 3. Generate Embeddings (One-Time Setup)

```bash
cd backend
npm run ingest
# This creates data/vector_store.json
# Typically takes 30â€“60 seconds for 10 documents
```

### 4. Start the Backend

```bash
cd backend
npm start
# Server runs on http://localhost:3001
```

### 5. Start the Frontend

```bash
cd frontend
npm run dev
# App runs on http://localhost:5173
```

### 6. Open the App

Navigate to [http://localhost:5173](http://localhost:5173) and start chatting!

---

## ğŸ”Œ API Reference

### `POST /api/chat`

**Request:**
```json
{
  "sessionId": "session_abc123",
  "message": "What is the refund policy?"
}
```

**Response:**
```json
{
  "reply": "Unused retainer balances are refunded within 14 business days of case closure...",
  "tokensUsed": 312,
  "retrievedChunks": 3,
  "similarityScores": [
    { "title": "Refund and Retainer Policy", "score": 0.8412 },
    { "title": "Consultation Fees and Billing", "score": 0.7231 },
    { "title": "Case Filing Procedure", "score": 0.5812 }
  ]
}
```

**Error Responses:**
| Status | Scenario |
|---|---|
| 400 | Missing/invalid sessionId or message |
| 401 | Invalid API key |
| 429 | Rate limit reached |
| 504 | LLM request timeout |
| 500 | Internal server error |

### `DELETE /api/session/:sessionId`
Clears the conversation history for a session.

### `GET /api/health`
Returns server status and chunk count.

---

## ğŸ§ª Sample Questions to Test

- "What are the consultation fees?"
- "How do I reset my password?" *(should trigger fallback â€” not in KB)*
- "What is the refund policy for retainers?"
- "Can I change my assigned attorney?"
- "What practice areas do you cover?"
- "How do I schedule an emergency consultation?"
- "What happens if I miss my appointment?"

---

## ğŸ“Š Evaluation Notes

| Area | Implementation |
|---|---|
| **RAG Architecture** | Full pipeline: ingest â†’ embed â†’ retrieve â†’ augment â†’ respond |
| **Embedding & Similarity** | OpenAI `text-embedding-3-small` + cosine similarity + 0.5 threshold |
| **LLM Integration** | GPT-4o-mini, temp=0.2, token logging, full error handling |
| **Prompt Design** | Grounded system prompt, fallback on no context, history-aware |
| **Frontend UI** | React + Vite, markdown rendering, loading states, session handling |
| **Code Quality** | Modular structure, error handling, comments, input validation |

---

*Built for the Rest the Case technical assignment. All knowledge base content is fictional and for demonstration purposes.*
